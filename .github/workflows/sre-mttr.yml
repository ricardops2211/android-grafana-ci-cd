# .github/workflows/sre-mttr.yml
name: sre-mttr-loki

on:
  push:
    branches: [ "sre-richie" ]
  workflow_dispatch:
    inputs:
      chaos_minutes:
        description: "Duración falla (min)"
        required: false
        default: "2"
      mttr_slo_seconds:
        description: "SLO MTTR (s)"
        required: false
        default: "300"

concurrency:
  group: sre-mttr-${{ github.ref }}
  cancel-in-progress: true

permissions:
  contents: read

jobs:
  mttr-lab:
    runs-on: self-hosted
    env:
      NAMESPACE: sre-demo
      DEPLOYMENT: api-demo
      APP_LABEL: api-demo
      RUN_ID: ${{ github.run_id }}

    steps:
      - uses: actions/checkout@v4

      # Defaults seguros para push/dispatch
      - name: Set defaults for chaos & SLO
        id: defs
        shell: bash
        run: |
          CHAOS="${{ github.event.inputs.chaos_minutes }}"
          SLO="${{ github.event.inputs.mttr_slo_seconds }}"
          [ -z "$CHAOS" ] && CHAOS=2
          [ -z "$SLO" ] && SLO=300
          echo "chaos=$CHAOS" >> "$GITHUB_OUTPUT"
          echo "slo=$SLO" >> "$GITHUB_OUTPUT"

      - name: kubectl setup
        uses: ./.github/actions/kubectl-setup
        with:
          kubeconfig_b64: ${{ secrets.KUBECONFIG_B64 }}

      - name: Helm repos & namespaces
        shell: bash
        run: |
          set -euo pipefail
          helm repo add grafana https://grafana.github.io/helm-charts
          helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
          helm repo update
          kubectl get ns observability >/dev/null 2>&1 || kubectl create ns observability
          kubectl get ns monitoring   >/dev/null 2>&1 || kubectl create ns monitoring
          kubectl get ns $NAMESPACE   >/dev/null 2>&1 || kubectl create ns $NAMESPACE

      # (Opcional pero útil en push): reconstruir y cargar la imagen local
      - name: Build & load app image
        if: ${{ github.event_name == 'push' }}
        shell: bash
        run: |
          set -euo pipefail
          docker build -t api-demo:local ./app
          kind load docker-image api-demo:local --name sre-demo

      # Render del values de Alertmanager (inyecta SLACK_WEBHOOK_URL)
      - name: Render monitoring values (Alertmanager Slack)
        env:
          SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}
        shell: bash
        run: |
          set -euo pipefail
          if command -v envsubst >/dev/null 2>&1; then
            RENDER=envsubst
          elif [ -x ./tools/envsubst ]; then
            RENDER=./tools/envsubst
          else
            echo "ERROR: envsubst no disponible. Instálalo en el host o incluye ./tools/envsubst en el repo." >&2
            exit 1
          fi
          (cd k8s && $RENDER < monitoring-values.yaml > monitoring-values.rendered.yaml)
          ls -l k8s/monitoring-values.rendered.yaml
          # Aviso si no se inyectó URL (secret vacío)
          grep -q "hooks.slack.com" k8s/monitoring-values.rendered.yaml || echo "::warning::No se detectó la URL de Slack en el values renderizado"

      - name: Install kube-prometheus-stack (AM/Prom)
        shell: bash
        run: |
          set -euo pipefail
          helm upgrade --install kps prometheus-community/kube-prometheus-stack \
            -n monitoring -f k8s/monitoring-values.rendered.yaml

      - name: Wait for Prometheus & Alertmanager to be ready (kps)
        shell: bash
        run: |
          set -euo pipefail

          # 1) Espera por CR (si reportan condición Available)
          kubectl -n monitoring wait --for=condition=Available --timeout=600s prometheus/kps-kube-prometheus-stack-prometheus || true
          kubectl -n monitoring wait --for=condition=Available --timeout=600s alertmanager/kps-kube-prometheus-stack-alertmanager || true

          # 2) Espera por StatefulSets reales (nombres correctos)
          kubectl -n monitoring rollout status statefulset/prometheus-kps-kube-prometheus-stack-prometheus --timeout=600s || true
          kubectl -n monitoring rollout status statefulset/alertmanager-kps-kube-prometheus-stack-alertmanager --timeout=600s || true

          # 3) Fallback: espera pods Ready por label
          kubectl -n monitoring wait pod -l app.kubernetes.io/name=prometheus,app.kubernetes.io/instance=kps --for=condition=Ready --timeout=600s || true
          kubectl -n monitoring wait pod -l app.kubernetes.io/name=alertmanager,app.kubernetes.io/instance=kps --for=condition=Ready --timeout=600s || true

          echo "---- Prometheus ----"
          kubectl -n monitoring get pods -l app.kubernetes.io/name=prometheus,app.kubernetes.io/instance=kps -o wide || true
          echo "---- Alertmanager ----"
          kubectl -n monitoring get pods -l app.kubernetes.io/name=alertmanager,app.kubernetes.io/instance=kps -o wide || true

      - name: Install Loki/Promtail/Grafana
        shell: bash
        run: |
          set -euo pipefail
          helm upgrade --install loki grafana/loki -n observability -f k8s/loki-values.yaml
          helm upgrade --install promtail grafana/promtail -n observability -f k8s/promtail-values.yaml
          helm upgrade --install grafana grafana/grafana -n observability -f k8s/grafana-values.yaml
          kubectl -n observability rollout status deploy/grafana --timeout=600s


      - name: Deploy app
        shell: bash
        run: |
          set -euo pipefail
          kubectl apply -f k8s/deployment.yaml
          kubectl -n $NAMESPACE rollout status deploy/$DEPLOYMENT --timeout=300s

      - name: Apply Grafana dashboard (ConfigMap sidecar)
        shell: bash
        run: |
          set -euo pipefail
          awk 'BEGIN{print "apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: dashboard-mttr\n  namespace: observability\n  labels:\n    grafana_dashboard: \"1\"\ndata:\n  sre-mttr-loki.json: |"} {print "    "$0}' k8s/grafana-dashboard-mttr.json > k8s/grafana-dashboard-configmap.yaml
          kubectl -n observability apply -f k8s/grafana-dashboard-configmap.yaml

      - name: Port-forward Loki
        uses: ./.github/actions/loki-port-forward
        with:
          namespace: observability

      - name: Grace period for Loki
        shell: bash
        run: sleep 2

      - name: Incident start
        id: start
        uses: ./.github/actions/loki-push
        with:
          type: incident_start
          run_id: ${{ env.RUN_ID }}
          app: ${{ env.APP_LABEL }}
          message: "incident start caused_by=chaos"
          loki_url: "http://127.0.0.1:3100"   


      - name: Chaos ON
        uses: ./.github/actions/chaos-toggle
        with:
          namespace: ${{ env.NAMESPACE }}
          deployment: ${{ env.DEPLOYMENT }}
          fail_rate: "1.0"

      - name: Hold chaos
        shell: bash
        run: |
          set -euo pipefail
          CHAOS=${{ steps.defs.outputs.chaos }}
          sleep $((60 * CHAOS))

      - name: Chaos OFF
        uses: ./.github/actions/chaos-toggle
        with:
          namespace: ${{ env.NAMESPACE }}
          deployment: ${{ env.DEPLOYMENT }}
          fail_rate: "0.0"

      - name: Wait stable
        uses: ./.github/actions/wait-stable
        with:
          namespace: ${{ env.NAMESPACE }}
          app_label: ${{ env.APP_LABEL }}

      - name: Incident end
        id: end
        uses: ./.github/actions/loki-push
        with:
          type: incident_end
          run_id: ${{ env.RUN_ID }}
          app: ${{ env.APP_LABEL }}
          message: "incident end status=recovered"
          loki_url: "http://127.0.0.1:3100"   

      - name: Calc MTTD/MTTR
        id: calc
        uses: ./.github/actions/loki-calc-mttr
        with:
          start_ns: ${{ steps.start.outputs.ts_ns }}
          end_ns:   ${{ steps.end.outputs.ts_ns }}
          query_error: '{app="${{ env.APP_LABEL }}"} |= "ERROR"'
          loki_url: "http://127.0.0.1:3100" 

      - name: Upload report
        uses: ./.github/actions/upload-report
        with:
          run_id:       ${{ env.RUN_ID }}
          mttr_seconds: ${{ steps.calc.outputs.mttr_seconds }}
          mttd_seconds: ${{ steps.calc.outputs.mttd_seconds }}

      - name: Enforce SLO
        uses: ./.github/actions/enforce-slo
        with:
          mttr_seconds: ${{ steps.calc.outputs.mttr_seconds }}
          slo_seconds:  ${{ steps.defs.outputs.slo }}

      - name: Cleanup port-forward
        if: always()
        shell: bash
        run: pkill -f "port-forward.*svc/loki" || true
